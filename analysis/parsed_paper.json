{
  "title": "t-RELOAD: A REinforcement Learning-based REcommendation",
  "authors": [
    "the players, which reduces in proportion of ordinal choice of"
  ],
  "venue": [
    "on Artificial Intelligence and Machine Learning Systems (AIMLSys-",
    "on knowledge",
    "on Robot",
    "on Machine",
    "6, 2 (2019), 00‚Äì00.",
    "on machine",
    "on",
    "on Machine Learning. PMLR, 4434‚Äì",
    "of Information Engineering &",
    "on Data Science & Management of",
    "on artificial",
    "on Machine",
    "on machine learning. PMLR, 1995‚Äì2003.",
    "on Machine Learning. 1201‚Äì1208.",
    "on",
    "on the Foundations"
  ],
  "year": [
    "2023",
    "2023",
    "2023",
    "2023",
    "2023",
    "2023",
    "2023",
    "2023",
    "2023",
    "2023",
    "2023",
    "2023",
    "2023",
    "2023",
    "2023",
    "2023",
    "2023",
    "2022",
    "2022",
    "2016",
    "2020",
    "2020",
    "2011",
    "2020",
    "2020",
    "2018",
    "2019",
    "2019",
    "2021",
    "2018",
    "2019",
    "2019",
    "2012",
    "2012",
    "2020",
    "2020",
    "2020",
    "2019",
    "2019",
    "2020",
    "2020",
    "2016",
    "2016",
    "2022",
    "2022",
    "2013",
    "2013",
    "2015",
    "2015",
    "2022",
    "2021",
    "2021",
    "2021",
    "2021",
    "2019",
    "2019",
    "2018",
    "2016",
    "2017",
    "2016",
    "2018",
    "2020"
  ],
  "abstract": "Games of skill provide an excellent source of entertainment to re-\nalize self-esteem, relaxation and social gratification. Engagement\nin online skill gaming platforms is however heavily dependent\non the outcomes and experience (e.g., wins/losses). A user can be-\nhave differently under different win/loss experience. An intense\nengagement can lead to potential demotivation and consequential\nchurn, while a lighter engagement can lead to more confidence for\nlonger sustenance‚Äîall depending on the outcomes. Generating a\nrelevant recommendation using reinforcement learning (RL) that\ncan also lead to high engagement (both intensity and duration)\nis non-trivial because: (i) an early exploration through online-RL\nusing a combined multi-objective reward can permanently hurt\nusers; and (ii) a simulation environment to evaluate RL policies\nis hard to model due to the (unknown) outcome-driven natural\nvolatility in user behaviour. This work addresses the question\n‚Äúhow can we leverage off-policy data for recommendation to solve\ncold-start problem while ensuring reward-driven optimality from\nplatform-perspective in outcome-based applications? ‚Äù. We introduce\nt-RELOAD: A REinforcement Learning-based REcommendation\nframework for Outcome-driven Application consisting of 3-layer-\nbased architecture: (i) off-policy data-collection (through already\ndeployed solution), (ii) offline training (using relevancy) and, (iii)\nonline exploration with turbo-reward (t-reward, using engagement).\nWe compare the performance of t-RELOAD with an XGBoost-based\nrecommendation system already in-place to capture the effective-\nness.\nACM Reference Format:\nDebanjan Sadhukhan, Sachin Kumar, Swarit Sankule, and Tridib Mukher-\njee. 2023. t-RELOAD: A REinforcement Learning-based REcommendation\nfor Outcome-driven Application. In Proceedings of The Third International\nConference on Artificial Intelligence and Machine Learning Systems (AIMLSys-\ntems 2023). ACM, Bangalore, India, 7 pages. https://doi.org/10.1145/nnnnnnn.\nnnnnnnn\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than ACM\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\nto post on servers or to redistribute to lists, requires prior specific permission and/or a\nfee. Request permissions from permissions@acm.org.\nAIMLSystems 2023, October 25‚Äì28, 2023, Bangalore, India\n¬© 2023 Association for Computing Machinery.\nhttps://doi.org/10.1145/nnnnnnn.nnnnnnn\n1",
  "contributions": null,
  "method_summary": null,
  "assumptions": null,
  "equations": null,
  "algorithms": {
    "Algorithm_1": "Offline training",
    "Algorithm_2": "Online training"
  },
  "datasets": [
    "ùê∑, and hence the exploration is not possible. If ùê∑fails to contain",
    "ùê∑covers",
    "is (1855131, 124) where total number of user-"
  ],
  "metrics": [
    "[16] as shown in Fig 2). The first-kind of users are chosen",
    "derivative dynamic time warping. Expert Systems with Applications 62 (2016),"
  ],
  "baselines": null,
  "hyperparams": {}
}